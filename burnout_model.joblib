import pandas as pd

df = pd.read_csv(r"C:\Users\ASUS\OneDrive\Desktop\miniproject\Share STUDENT BURNOUT LEVEL PREDICTION (Responses) - Form responses .csv")   # use your dataset name
pd.set_option('display.max_rows', None)
print(df)
print(df.info())
print(df.shape)
print(df.describe())
print(df.isnull().sum())
# Drop Name column
df = df.drop("Name", axis=1)

# Encode categorical features
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

for col in ["Study Hours", "Sleep Hours", "Stress"]:
    df[col] = le.fit_transform(df[col])

# Simple burnout scoring system (you can adjust weights)
df["burnout_score"] = (
    df["Study Hours"] * 2 +
    (10 - df["Sleep Hours"]) +   # less sleep â†’ higher burnout
    df["Stress"] * 3 +
    (10 - df["CGPA (Out of 10)"]) +
    (100 - df["Attendance Percentage"]) / 10
)

# Convert burnout_score to classes
def classify(score):
    if score <= df["burnout_score"].quantile(0.33):
        return "Low"
    elif score <= df["burnout_score"].quantile(0.66):
        return "Medium"
    else:
        return "High"

df["burnout_class"] = df["burnout_score"].apply(classify)

print(df.head())
print(df["burnout_class"].value_counts())

#preprocessing the data
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Drop score column (not needed for training)
df = df.drop("burnout_score", axis=1)

# Features and target
X = df.drop("burnout_class", axis=1)
y = df["burnout_class"]

# Encode categorical variables
categorical_cols = ["Study Hours", "Sleep Hours", "Stress"]
X = pd.get_dummies(X, columns=categorical_cols)

# Encode target labels (Low=0, Medium=1, High=2)
le = LabelEncoder()
y = le.fit_transform(y)

# Scale numeric features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y distribution:", pd.Series(y).value_counts())

#traing ml model
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# 1. Logistic Regression
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train, y_train)
y_pred_log = log_reg.predict(X_test)

print("Logistic Regression Results:")
print(classification_report(y_test, y_pred_log))
print(confusion_matrix(y_test, y_pred_log))

# 2. Decision Tree
tree = DecisionTreeClassifier(random_state=42)
tree.fit(X_train, y_train)
y_pred_tree = tree.predict(X_test)

print("\nDecision Tree Results:")
print(classification_report(y_test, y_pred_tree))
print(confusion_matrix(y_test, y_pred_tree))

# 3. Random Forest
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

print("\nRandom Forest Results:")
print(classification_report(y_test, y_pred_rf))
print(confusion_matrix(y_test, y_pred_rf))

# Encode target labels for y
from sklearn.preprocessing import LabelEncoder

le_y = LabelEncoder()        # <-- define le_y here
y = le_y.fit_transform(y)    # <-- fit y


#confusion matrix for visualization
import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay

# Plot confusion matrix for Random Forest (best model usually)
ConfusionMatrixDisplay.from_estimator(rf, X_test, y_test, display_labels=le_y.classes_, cmap="Blues")
plt.title("Confusion Matrix - Random Forest")
plt.show()

#random forest visualization
import numpy as np

feature_importances = rf.feature_importances_
features = X.columns  # after encoding

# Sort features by importance
indices = np.argsort(feature_importances)[::-1]

plt.figure(figsize=(8,6))
plt.bar(range(len(features)), feature_importances[indices], align="center")
plt.xticks(range(len(features)), features[indices], rotation=45, ha="right")
plt.title("Feature Importance - Random Forest")
plt.tight_layout()
plt.show()

#metrics recap
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Predictions
y_pred = rf.predict(X_test)

# Accuracy
acc = accuracy_score(y_test, y_pred)

# Precision, Recall, F1 (macro = average across Low/Medium/High)
prec = precision_score(y_test, y_pred, average="macro")
rec = recall_score(y_test, y_pred, average="macro")
f1 = f1_score(y_test, y_pred, average="macro")

print("ðŸ“Š Metrics Recap (Random Forest)")
print(f"Accuracy : {acc:.2f}")
print(f"Precision: {prec:.2f}")
print(f"Recall   : {rec:.2f}")
print(f"F1 Score : {f1:.2f}")

#saving the model
import joblib

# Save the trained Random Forest model
joblib.dump(rf, "burnout_model.joblib")

# Save the scaler too (needed for preprocessing later)
joblib.dump(scaler, "scaler.joblib")

print("âœ… Model and scaler saved successfully!")

#downloading the model
from google.colab import files
files.download("burnout_model.joblib")
files.download("scaler.joblib")

# Reload model
loaded_model = joblib.load("burnout_model.joblib")
loaded_scaler = joblib.load("scaler.joblib")

# Example: make a new prediction
sample = X_test[0].reshape(1, -1)  # take one student example
prediction = loaded_model.predict(sample)

print("Predicted burnout class:", le.inverse_transform(prediction)[0])

#frontend
